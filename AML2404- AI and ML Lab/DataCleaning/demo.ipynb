{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from textblob import TextBlob\n",
    "\n",
    "def tokenization(text):\n",
    "    # REMOVING PUNCTUATIONS\n",
    "    inputText = re.sub(r'[^\\w\\s]','',text)\n",
    "    inputText = re.sub(r'\\d+', '', inputText)\n",
    "    lowerString = inputText.lower()\n",
    "    wordToknized = word_tokenize(lowerString)\n",
    "    return wordToknized\n",
    "\n",
    "def removeStopwords(tokens):\n",
    "    wordList = stopwords.words('english')\n",
    "    filtered_words = [word for word in tokens if word not in wordList]\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "def wordlemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    verb = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    adverb = ['RB', 'RBR', 'RBS' ]\n",
    "    adjective = ['JJ','JJR','JJS']\n",
    "    for word in tokens:\n",
    "        postTag = dict(pos_tag([word]))\n",
    "        if postTag[word] in noun:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif postTag[word] in verb:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "        elif postTag[word] in adjective:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, pos='a'))\n",
    "        elif postTag[word] in adverb:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, pos='r'))\n",
    "        else:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "def correct_spelling(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = str(blob.correct())\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ['this is a test from microsoft','it is widley used application for testing as well as development']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m processed_data \u001b[39m=\u001b[39m df\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m func \u001b[39min\u001b[39;00m pipeline_functions:\n\u001b[1;32m----> 5\u001b[0m     processed_data \u001b[39m=\u001b[39m func(processed_data)\n\u001b[0;32m      7\u001b[0m final_result \u001b[39m=\u001b[39m processed_data\n\u001b[0;32m      8\u001b[0m final_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(final_result)\n",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m, in \u001b[0;36mtokenization\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenization\u001b[39m(text):\n\u001b[0;32m      9\u001b[0m     \u001b[39m# REMOVING PUNCTUATIONS\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     inputText \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m[^\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms]\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,text)\n\u001b[0;32m     11\u001b[0m     inputText \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, inputText)\n\u001b[0;32m     12\u001b[0m     lowerString \u001b[39m=\u001b[39m inputText\u001b[39m.\u001b[39mlower()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\re.py:209\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    203\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msub(repl, string, count)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# pipeline_functions = [tokenization]\n",
    "# final_synopsis =[]\n",
    "# processed_data = df\n",
    "# for func in pipeline_functions:\n",
    "#     processed_data = func(processed_data)\n",
    "\n",
    "# final_result = processed_data\n",
    "# final_text = ' '.join(final_result)\n",
    "# final_synopsis.append(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
