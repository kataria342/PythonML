{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.corpus import webtext\n",
    "from autocorrect import Speller\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.collections import *\n",
    "from autocorrect import Speller\n",
    "from pattern.en import suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('jump.n.01'), Synset('leap.n.02'), Synset('jump.n.03'), Synset('startle.n.01'), Synset('jump.n.05'), Synset('jump.n.06'), Synset('jump.v.01'), Synset('startle.v.02'), Synset('jump.v.03'), Synset('jump.v.04'), Synset('leap_out.v.01'), Synset('jump.v.06'), Synset('rise.v.11'), Synset('jump.v.08'), Synset('derail.v.02'), Synset('chute.v.01'), Synset('jump.v.11'), Synset('jumpstart.v.01'), Synset('jump.v.13'), Synset('leap.v.02'), Synset('alternate.v.01')]\n"
     ]
    }
   ],
   "source": [
    "syns = wordnet.synsets('Jump')\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump.n.01\n",
      "jump\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].name())\n",
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21052631578947367\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('brain.n.01')\n",
    "\n",
    "print(w1.wup_similarity(w2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Spell Checker Techniques**\n",
    "\n",
    "#### **Important steps involved**\n",
    "##### ***1. fixing word lengthening***\n",
    "##### ***2. perform text preprocessing***\n",
    "##### ***3. perform spell fixing***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***regex pre-trained complier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booook : book\n",
      "bboookkk : bbookk\n",
      "cooool : cool\n",
      "ccooolll : ccooll\n"
     ]
    }
   ],
   "source": [
    "def remove_length(text):        \n",
    "    patt = re.compile(r\"(.)\\1{2,}\")\n",
    "    return patt.sub(r\"\\1\\1\",text)\n",
    "\n",
    "inputString = ['booook','bboookkk','cooool','ccooolll']  \n",
    "for i in inputString:\n",
    "    print(i,':',remove_length(text=i))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***spell pre-trained machine model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mussage : message\n",
      "survice : service\n",
      "hte : the\n",
      "caaaar : aaaaaa\n"
     ]
    }
   ],
   "source": [
    "def fixSpellingSpell(text):\n",
    "    return spell(text)\n",
    "\n",
    "spell = Speller(lang='en')    \n",
    "inputString = ['mussage','survice','hte','caaaar']  \n",
    "for i in inputString:\n",
    "    print(i,':',fixSpellingSpell(text=i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***suggest pre-trained machine model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mussage : [('message', 0.6216216216216216), ('massage', 0.3783783783783784)]\n",
      "survice : [('service', 0.9253112033195021), ('survive', 0.07468879668049792)]\n",
      "hte : [('the', 0.8653201565642368), ('he', 0.13408515883485067), ('ate', 0.00022706139307570876), ('hate', 0.0002162489457863893), ('hue', 0.00012974936747183358), ('te', 1.0812447289319465e-05), ('htm', 1.0812447289319465e-05)]\n",
      "caaaaaaar : [('caaaaaaar', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "def fixSpellingSuggest(text):\n",
    "    return suggest(text)\n",
    "  \n",
    "inputString = ['mussage','survice','hte','caaaaaaar']  \n",
    "for i in inputString:\n",
    "    print(i,':',fixSpellingSuggest(text=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ths neww abot eerth', 0.0)]\n",
      "[('the', 0.9481216457960644), ('this', 0.048134677581774456), ('thus', 0.002511580518665071), ('th', 0.0006042009738298049), ('thy', 0.000556812662156879), ('tis', 5.923538959115734e-05), ('tss', 1.1847077918231468e-05)]\n",
      "[('new', 0.8439024390243902), ('news', 0.15609756097560976)]\n",
      "[('about', 0.9966711051930759), ('cabot', 0.0019973368841544607), ('abort', 0.0006657789613848203), ('abbot', 0.0006657789613848203)]\n",
      "[('earth', 0.9831932773109243), ('berth', 0.01680672268907563)]\n"
     ]
    }
   ],
   "source": [
    "print(suggest('ths neww abot eerth'))\n",
    "print(suggest('ths'))\n",
    "print(suggest('neww'))\n",
    "print(suggest('abot'))\n",
    "print(suggest('eerth'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of words:  1583820\n",
      "Review Categories:  ['neg', 'pos']\n",
      "File Ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt']\n"
     ]
    }
   ],
   "source": [
    "print('length of words: ', len(movie_reviews.words()))\n",
    "print('Review Categories: ', movie_reviews.categories())\n",
    "print('File Ids:',movie_reviews.fileids()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    1000\n",
       "pos    1000\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs1 = [(list(movie_reviews.words(fileid)), category) \n",
    "        for category in movie_reviews.categories() \n",
    "        for fileid in movie_reviews.fileids(category)\n",
    "        ]\n",
    "df1 = pd.DataFrame(docs1)\n",
    "df1[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative reviews: 1000\n",
      "Number of positive reviews: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of negative reviews:\", len(movie_reviews.fileids(categories=['neg'])))\n",
    "print(\"Number of positive reviews:\", len(movie_reviews.fileids(categories=['pos'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length before removing punctuations :  7810519\n",
      "Text length after removing punctuations :  7559896\n"
     ]
    }
   ],
   "source": [
    "text = \" \".join(movie_reviews.words())\n",
    "\n",
    "import string\n",
    "text_filtered = text.translate(str.maketrans('','',string.punctuation))\n",
    "print('Text length before removing punctuations : ', len(text))\n",
    "print('Text length after removing punctuations : ', len(text_filtered))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length before removing stopwords :  1337085\n",
      "Text length after removing stopwords :  708475\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text_filtered)\n",
    "\n",
    "word_filtered = [w.lower() for w in tokens if w.lower() not in stopwords]\n",
    "\n",
    "print('Text length before removing stopwords : ', len(tokens))\n",
    "print('Text length after removing stopwords : ', len(word_filtered))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "1. Dictionary of frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 9519), ('one', 5853), ('movie', 5774), ('like', 3690), ('even', 2565), ('good', 2411), ('time', 2411), ('story', 2170), ('would', 2110), ('much', 2050), ('character', 2020), ('also', 1967), ('get', 1949), ('two', 1912), ('well', 1906), ('characters', 1859), ('first', 1836), ('see', 1749), ('way', 1693), ('make', 1642)]\n",
      "39295\n"
     ]
    }
   ],
   "source": [
    "counter_dict = nltk.FreqDist(word_filtered)\n",
    "print(counter_dict.most_common(20))\n",
    "print(len(counter_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [(list(movie_reviews.words(fileid)), category) \n",
    "        for category in movie_reviews.categories() \n",
    "        for fileid in movie_reviews.fileids(category)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', 'review', 'damn', 'that', 'y2k', 'bug', '.', 'it', \"'\", 's', 'got', 'a', 'head', 'start', 'in', 'this', 'movie', 'starring', 'jamie', 'lee', 'curtis', 'and', 'another', 'baldwin', 'brother', '(', 'william', 'this', 'time', ')', 'in', 'a', 'story', 'regarding', 'a', 'crew', 'of', 'a', 'tugboat', 'that', 'comes', 'across', 'a', 'deserted', 'russian', 'tech', 'ship', 'that', 'has', 'a', 'strangeness', 'to', 'it', 'when', 'they', 'kick', 'the', 'power', 'back', 'on', '.', 'little', 'do', 'they', 'know', 'the', 'power', 'within', '.', '.', '.', 'going', 'for', 'the', 'gore', 'and', 'bringing', 'on', 'a', 'few', 'action', 'sequences', 'here', 'and', 'there', ',', 'virus', 'still', 'feels', 'very', 'empty', ',', 'like', 'a', 'movie', 'going', 'for', 'all', 'flash', 'and', 'no', 'substance', '.', 'we', 'don', \"'\", 't', 'know', 'why', 'the', 'crew', 'was', 'really', 'out', 'in', 'the', 'middle', 'of', 'nowhere', ',', 'we', 'don', \"'\", 't', 'know', 'the', 'origin', 'of', 'what', 'took', 'over', 'the', 'ship', '(', 'just', 'that', 'a', 'big', 'pink', 'flashy', 'thing', 'hit', 'the', 'mir', ')', ',', 'and', ',', 'of', 'course', ',', 'we', 'don', \"'\", 't', 'know', 'why', 'donald', 'sutherland', 'is', 'stumbling', 'around', 'drunkenly', 'throughout', '.', 'here', ',', 'it', \"'\", 's', 'just', '\"', 'hey', ',', 'let', \"'\", 's', 'chase', 'these', 'people', 'around', 'with', 'some', 'robots', '\"', '.', 'the', 'acting', 'is', 'below', 'average', ',', 'even', 'from', 'the', 'likes', 'of', 'curtis', '.', 'you', \"'\", 're', 'more', 'likely', 'to', 'get', 'a', 'kick', 'out', 'of', 'her', 'work', 'in', 'halloween', 'h20', '.', 'sutherland', 'is', 'wasted', 'and', 'baldwin', ',', 'well', ',', 'he', \"'\", 's', 'acting', 'like', 'a', 'baldwin', ',', 'of', 'course', '.', 'the', 'real', 'star', 'here', 'are', 'stan', 'winston', \"'\", 's', 'robot', 'design', ',', 'some', 'schnazzy', 'cgi', ',', 'and', 'the', 'occasional', 'good', 'gore', 'shot', ',', 'like', 'picking', 'into', 'someone', \"'\", 's', 'brain', '.', 'so', ',', 'if', 'robots', 'and', 'body', 'parts', 'really', 'turn', 'you', 'on', ',', 'here', \"'\", 's', 'your', 'movie', '.', 'otherwise', ',', 'it', \"'\", 's', 'pretty', 'much', 'a', 'sunken', 'ship', 'of', 'a', 'movie', '.'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['capsule',\n",
       " ':',\n",
       " 'the',\n",
       " 'much',\n",
       " 'anticipated',\n",
       " 're',\n",
       " '-',\n",
       " 'adaptation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pierre',\n",
       " 'boulle',\n",
       " 'novel',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'the',\n",
       " 'screen',\n",
       " 'as',\n",
       " 'a',\n",
       " 'dark',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'dreary',\n",
       " 'film',\n",
       " 'with',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'chases',\n",
       " 'and',\n",
       " 'fighting',\n",
       " ',',\n",
       " 'but',\n",
       " 'very',\n",
       " 'little',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'visually',\n",
       " 'there',\n",
       " 'is',\n",
       " 'much',\n",
       " 'to',\n",
       " 'like',\n",
       " 'about',\n",
       " 'this',\n",
       " 'version',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'approach',\n",
       " 'is',\n",
       " 'to',\n",
       " 'take',\n",
       " 'an',\n",
       " 'adventure',\n",
       " 'after',\n",
       " 'the',\n",
       " 'style',\n",
       " 'of',\n",
       " 'gulliver',\n",
       " \"'\",\n",
       " 's',\n",
       " 'travels',\n",
       " 'and',\n",
       " 'treat',\n",
       " 'it',\n",
       " 'as',\n",
       " 'an',\n",
       " 'action',\n",
       " 'film',\n",
       " '.',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'a',\n",
       " 'film',\n",
       " 'without',\n",
       " 'much',\n",
       " 'center',\n",
       " '.',\n",
       " ',',\n",
       " '0',\n",
       " '(',\n",
       " '-',\n",
       " '4',\n",
       " 'to',\n",
       " '+',\n",
       " '4',\n",
       " ')',\n",
       " 'pierre',\n",
       " 'boulle',\n",
       " ',',\n",
       " 'author',\n",
       " 'of',\n",
       " 'the',\n",
       " 'bridge',\n",
       " 'on',\n",
       " 'the',\n",
       " 'river',\n",
       " 'kwai',\n",
       " ',',\n",
       " 'wrote',\n",
       " 'planet',\n",
       " 'of',\n",
       " 'the',\n",
       " 'apes',\n",
       " '(',\n",
       " 'a',\n",
       " '.',\n",
       " 'k',\n",
       " '.',\n",
       " 'a',\n",
       " '.',\n",
       " 'monkey',\n",
       " 'planet',\n",
       " ')',\n",
       " ',',\n",
       " 'the',\n",
       " 'novel',\n",
       " ',',\n",
       " 'as',\n",
       " 'a',\n",
       " 'social',\n",
       " 'satire',\n",
       " '.',\n",
       " 'it',\n",
       " 'reads',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'like',\n",
       " 'a',\n",
       " 'fifth',\n",
       " 'book',\n",
       " 'of',\n",
       " 'gulliver',\n",
       " \"'\",\n",
       " 's',\n",
       " 'travels',\n",
       " '.',\n",
       " 'humans',\n",
       " 'discover',\n",
       " 'a',\n",
       " 'planet',\n",
       " 'in',\n",
       " 'which',\n",
       " 'the',\n",
       " 'roles',\n",
       " 'of',\n",
       " 'apes',\n",
       " 'and',\n",
       " 'humans',\n",
       " 'have',\n",
       " 'been',\n",
       " 'reversed',\n",
       " ',',\n",
       " 'not',\n",
       " 'unlike',\n",
       " 'the',\n",
       " 'roles',\n",
       " 'of',\n",
       " 'horses',\n",
       " 'and',\n",
       " 'humans',\n",
       " 'on',\n",
       " 'jonathan',\n",
       " 'swift',\n",
       " \"'\",\n",
       " 's',\n",
       " 'island',\n",
       " 'of',\n",
       " 'the',\n",
       " 'houyhnhnms',\n",
       " '.',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'moves',\n",
       " 'somewhat',\n",
       " 'slowly',\n",
       " 'to',\n",
       " 'create',\n",
       " 'some',\n",
       " 'suspense',\n",
       " 'in',\n",
       " 'revealing',\n",
       " 'all',\n",
       " 'the',\n",
       " 'things',\n",
       " 'most',\n",
       " 'film',\n",
       " 'fans',\n",
       " 'know',\n",
       " 'to',\n",
       " 'be',\n",
       " 'true',\n",
       " 'about',\n",
       " 'the',\n",
       " 'nature',\n",
       " 'of',\n",
       " 'the',\n",
       " 'planet',\n",
       " '.',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'me',\n",
       " 'there',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'statement',\n",
       " 'about',\n",
       " 'human',\n",
       " 'cruelty',\n",
       " 'to',\n",
       " 'animals',\n",
       " ',',\n",
       " 'but',\n",
       " 'perhaps',\n",
       " 'i',\n",
       " 'was',\n",
       " 'just',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'that',\n",
       " '.',\n",
       " 'when',\n",
       " 'rod',\n",
       " 'serling',\n",
       " 'adapted',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'into',\n",
       " 'a',\n",
       " 'film',\n",
       " 'released',\n",
       " 'in',\n",
       " '1968',\n",
       " ',',\n",
       " 'he',\n",
       " 'added',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'serling',\n",
       " 'touches',\n",
       " ',',\n",
       " 'familiar',\n",
       " 'from',\n",
       " 'episodes',\n",
       " 'of',\n",
       " 'the',\n",
       " 'twilight',\n",
       " 'zone',\n",
       " 'and',\n",
       " 'changed',\n",
       " 'the',\n",
       " 'ending',\n",
       " 'to',\n",
       " 'make',\n",
       " 'it',\n",
       " 'more',\n",
       " 'serling',\n",
       " '-',\n",
       " 'ish',\n",
       " '.',\n",
       " 'the',\n",
       " 'final',\n",
       " 'irony',\n",
       " 'of',\n",
       " 'the',\n",
       " 'original',\n",
       " 'version',\n",
       " 'has',\n",
       " 'become',\n",
       " 'film',\n",
       " 'history',\n",
       " '.',\n",
       " 'without',\n",
       " 'it',\n",
       " 'there',\n",
       " 'could',\n",
       " 'never',\n",
       " 'have',\n",
       " 'been',\n",
       " 'a',\n",
       " '\"',\n",
       " 'planet',\n",
       " 'of',\n",
       " 'the',\n",
       " 'apes',\n",
       " '\"',\n",
       " 'film',\n",
       " 'series',\n",
       " '.',\n",
       " 'i',\n",
       " 'can',\n",
       " 'surmise',\n",
       " 'only',\n",
       " 'that',\n",
       " 'serling',\n",
       " 'ran',\n",
       " 'into',\n",
       " 'serious',\n",
       " 'script',\n",
       " 'problems',\n",
       " 'in',\n",
       " 'how',\n",
       " 'to',\n",
       " 'handle',\n",
       " 'the',\n",
       " 'tricky',\n",
       " 'question',\n",
       " 'of',\n",
       " 'language',\n",
       " '.',\n",
       " 'in',\n",
       " 'the',\n",
       " 'book',\n",
       " 'the',\n",
       " 'apes',\n",
       " 'had',\n",
       " 'their',\n",
       " 'own',\n",
       " 'language',\n",
       " 'and',\n",
       " 'the',\n",
       " 'human',\n",
       " 'eventually',\n",
       " 'learned',\n",
       " 'that',\n",
       " 'language',\n",
       " '.',\n",
       " 'that',\n",
       " 'could',\n",
       " 'have',\n",
       " 'been',\n",
       " 'done',\n",
       " 'in',\n",
       " 'the',\n",
       " 'film',\n",
       " ',',\n",
       " 'but',\n",
       " 'that',\n",
       " 'would',\n",
       " 'have',\n",
       " 'required',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'film',\n",
       " 'to',\n",
       " 'be',\n",
       " 'subtitled',\n",
       " 'for',\n",
       " 'the',\n",
       " 'non',\n",
       " '-',\n",
       " 'ape',\n",
       " '-',\n",
       " 'speaking',\n",
       " '.',\n",
       " 'serling',\n",
       " 'avoided',\n",
       " 'this',\n",
       " 'by',\n",
       " 'having',\n",
       " 'the',\n",
       " 'apes',\n",
       " 'speak',\n",
       " 'english',\n",
       " 'and',\n",
       " ',',\n",
       " 'of',\n",
       " 'course',\n",
       " ',',\n",
       " 'there',\n",
       " 'is',\n",
       " 'some',\n",
       " 'justification',\n",
       " 'for',\n",
       " 'that',\n",
       " 'by',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " '.',\n",
       " 'justifying',\n",
       " 'why',\n",
       " 'the',\n",
       " 'apes',\n",
       " 'spoke',\n",
       " 'english',\n",
       " 'may',\n",
       " 'have',\n",
       " 'even',\n",
       " 'been',\n",
       " 'the',\n",
       " 'inspiration',\n",
       " 'for',\n",
       " 'his',\n",
       " 'surprise',\n",
       " 'ending',\n",
       " '.',\n",
       " 'but',\n",
       " 'serling',\n",
       " 'never',\n",
       " 'tackles',\n",
       " 'the',\n",
       " 'all',\n",
       " '-',\n",
       " 'important',\n",
       " 'question',\n",
       " 'of',\n",
       " 'why',\n",
       " 'a',\n",
       " 'supposedly',\n",
       " 'intelligent',\n",
       " 'human',\n",
       " 'never',\n",
       " 'shows',\n",
       " 'any',\n",
       " 'curiosity',\n",
       " 'or',\n",
       " 'even',\n",
       " 'surprise',\n",
       " 'that',\n",
       " 'the',\n",
       " 'apes',\n",
       " 'speak',\n",
       " 'his',\n",
       " 'own',\n",
       " 'language',\n",
       " ',',\n",
       " 'a',\n",
       " 'language',\n",
       " 'they',\n",
       " 'had',\n",
       " 'no',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'ever',\n",
       " 'hear',\n",
       " '.',\n",
       " 'few',\n",
       " 'viewers',\n",
       " 'questioned',\n",
       " 'this',\n",
       " 'serious',\n",
       " 'plot',\n",
       " 'hole',\n",
       " ',',\n",
       " 'however',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'film',\n",
       " 'has',\n",
       " 'become',\n",
       " 'well',\n",
       " 'respected',\n",
       " 'in',\n",
       " 'cinema',\n",
       " 'history',\n",
       " '.',\n",
       " 'partial',\n",
       " 'credit',\n",
       " 'at',\n",
       " 'least',\n",
       " 'should',\n",
       " 'go',\n",
       " 'to',\n",
       " 'jerry',\n",
       " 'goldsmith',\n",
       " 'whose',\n",
       " 'extremely',\n",
       " 'inventive',\n",
       " 'score',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'goldsmith',\n",
       " \"'\",\n",
       " 's',\n",
       " 'best',\n",
       " 'if',\n",
       " 'not',\n",
       " 'his',\n",
       " 'best',\n",
       " '.',\n",
       " 'when',\n",
       " 'the',\n",
       " 'film',\n",
       " \"'\",\n",
       " 's',\n",
       " 'success',\n",
       " 'called',\n",
       " 'for',\n",
       " 'sequels',\n",
       " ',',\n",
       " 'the',\n",
       " 'filmmakers',\n",
       " 'turned',\n",
       " 'up',\n",
       " 'the',\n",
       " 'violence',\n",
       " 'and',\n",
       " 'they',\n",
       " 'added',\n",
       " 'well',\n",
       " '-',\n",
       " 'intentioned',\n",
       " ',',\n",
       " 'though',\n",
       " 'not',\n",
       " 'very',\n",
       " 'subtle',\n",
       " ',',\n",
       " 'political',\n",
       " 'messages',\n",
       " 'about',\n",
       " 'what',\n",
       " 'was',\n",
       " 'happening',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'of',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'and',\n",
       " '1970s',\n",
       " '.',\n",
       " 'while',\n",
       " 'the',\n",
       " 'first',\n",
       " 'film',\n",
       " 'had',\n",
       " 'a',\n",
       " 'little',\n",
       " 'shooting',\n",
       " 'of',\n",
       " 'guns',\n",
       " 'and',\n",
       " 'what',\n",
       " 'was',\n",
       " 'there',\n",
       " 'seemed',\n",
       " 'a',\n",
       " 'little',\n",
       " 'half',\n",
       " '-',\n",
       " 'hearted',\n",
       " ',',\n",
       " 'by',\n",
       " 'the',\n",
       " 'second',\n",
       " 'film',\n",
       " ',',\n",
       " 'beneath',\n",
       " 'the',\n",
       " 'planet',\n",
       " 'of',\n",
       " 'the',\n",
       " 'apes',\n",
       " ',',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'good',\n",
       " 'deal',\n",
       " 'more',\n",
       " 'violence',\n",
       " 'and',\n",
       " 'from',\n",
       " 'that',\n",
       " 'point',\n",
       " 'on',\n",
       " 'the',\n",
       " 'series',\n",
       " 'had',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'violence',\n",
       " 'and',\n",
       " 'chases',\n",
       " '.',\n",
       " 'the',\n",
       " 'series',\n",
       " 'concluded',\n",
       " 'with',\n",
       " 'battle',\n",
       " 'for',\n",
       " 'the',\n",
       " 'planet',\n",
       " 'of',\n",
       " 'the',\n",
       " 'apes',\n",
       " 'in',\n",
       " '1973',\n",
       " '.',\n",
       " 'now',\n",
       " 'director',\n",
       " 'tim',\n",
       " 'burton',\n",
       " 'tries',\n",
       " 'his',\n",
       " 'hand',\n",
       " 'at',\n",
       " 'adapting',\n",
       " 'the',\n",
       " 'original',\n",
       " 'book',\n",
       " 'again',\n",
       " '.',\n",
       " 'for',\n",
       " 'those',\n",
       " 'who',\n",
       " 'thought',\n",
       " 'that',\n",
       " 'the',\n",
       " '1968',\n",
       " 'version',\n",
       " 'was',\n",
       " 'not',\n",
       " 'very',\n",
       " 'faithful',\n",
       " 'to',\n",
       " 'the',\n",
       " 'book',\n",
       " ',',\n",
       " 'burton',\n",
       " \"'\",\n",
       " 's',\n",
       " 'new',\n",
       " 'version',\n",
       " 'is',\n",
       " 'even',\n",
       " 'less',\n",
       " 'faithful',\n",
       " '.',\n",
       " 'first',\n",
       " ',',\n",
       " 'he',\n",
       " 'does',\n",
       " 'not',\n",
       " 'really',\n",
       " 'reverse',\n",
       " 'the',\n",
       " 'roles',\n",
       " 'of',\n",
       " 'the',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'the',\n",
       " 'apes',\n",
       " '.',\n",
       " 'he',\n",
       " 'has',\n",
       " 'them',\n",
       " 'both',\n",
       " 'be',\n",
       " 'intelligent',\n",
       " ',',\n",
       " 'articulate',\n",
       " 'races',\n",
       " 'battling',\n",
       " 'for',\n",
       " 'a',\n",
       " 'dominance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'planet',\n",
       " 'currently',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hands',\n",
       " ',',\n",
       " 'uh',\n",
       " ',',\n",
       " 'make',\n",
       " 'that',\n",
       " 'paws',\n",
       " ',',\n",
       " 'of',\n",
       " 'the',\n",
       " 'apes',\n",
       " '.',\n",
       " 'that',\n",
       " 'could',\n",
       " 'be',\n",
       " 'a',\n",
       " 'good',\n",
       " 'story',\n",
       " 'too',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'planet',\n",
       " 'of',\n",
       " 'the',\n",
       " 'apes',\n",
       " '.',\n",
       " 'as',\n",
       " 'with',\n",
       " 'the',\n",
       " 'mission',\n",
       " 'impossible',\n",
       " 'films',\n",
       " 'and',\n",
       " 'so',\n",
       " 'many',\n",
       " 'other',\n",
       " 'cinematic',\n",
       " 'homages',\n",
       " 'to',\n",
       " 'the',\n",
       " 'third',\n",
       " 'quarter',\n",
       " 'of',\n",
       " 'the',\n",
       " 'last',\n",
       " 'century',\n",
       " ',',\n",
       " 'the',\n",
       " 'title',\n",
       " 'makes',\n",
       " 'promises',\n",
       " 'that',\n",
       " 'the',\n",
       " 'filmmakers',\n",
       " 'have',\n",
       " 'no',\n",
       " 'intention',\n",
       " 'of',\n",
       " 'honoring',\n",
       " '.',\n",
       " 'in',\n",
       " '2029',\n",
       " 'leo',\n",
       " 'davidson',\n",
       " '(',\n",
       " 'mark',\n",
       " 'wahlberg',\n",
       " ',',\n",
       " 'not',\n",
       " 'this',\n",
       " 'world',\n",
       " \"'\",\n",
       " 's',\n",
       " 'most',\n",
       " 'expressive',\n",
       " 'actor',\n",
       " ')',\n",
       " 'works',\n",
       " 'on',\n",
       " 'a',\n",
       " 'space',\n",
       " 'station',\n",
       " 'increasing',\n",
       " 'the',\n",
       " 'intelligence',\n",
       " 'and',\n",
       " 'usefulness',\n",
       " 'of',\n",
       " 'apes',\n",
       " '.',\n",
       " 'then',\n",
       " 'a',\n",
       " 'convenient',\n",
       " 'time',\n",
       " 'storm',\n",
       " 'sweeps',\n",
       " 'him',\n",
       " 'up',\n",
       " 'wizard',\n",
       " '-',\n",
       " 'of',\n",
       " '-',\n",
       " 'oz',\n",
       " '-',\n",
       " 'fashion',\n",
       " 'and',\n",
       " 'drops',\n",
       " 'him',\n",
       " 'on',\n",
       " 'an',\n",
       " 'alien',\n",
       " 'planet',\n",
       " '.',\n",
       " '(',\n",
       " 'yes',\n",
       " ',',\n",
       " 'he',\n",
       " 'survives',\n",
       " 'this',\n",
       " 'storm',\n",
       " ',',\n",
       " 'but',\n",
       " 'then',\n",
       " 'no',\n",
       " 'storm',\n",
       " 'is',\n",
       " 'perfect',\n",
       " '.',\n",
       " ')',\n",
       " 'he',\n",
       " 'quickly',\n",
       " 'finds',\n",
       " ',',\n",
       " 'not',\n",
       " 'greatly',\n",
       " 'to',\n",
       " 'any',\n",
       " 'surprise',\n",
       " 'he',\n",
       " 'shows',\n",
       " ',',\n",
       " 'that',\n",
       " 'on',\n",
       " 'this',\n",
       " 'planet',\n",
       " 'apes',\n",
       " 'rule',\n",
       " 'and',\n",
       " 'humans',\n",
       " 'drool',\n",
       " ',',\n",
       " 'but',\n",
       " 'everybody',\n",
       " 'talks',\n",
       " '.',\n",
       " 'and',\n",
       " 'the',\n",
       " 'language',\n",
       " 'they',\n",
       " 'talk',\n",
       " 'is',\n",
       " 'earth',\n",
       " '-',\n",
       " 'english',\n",
       " '.',\n",
       " 'apparently',\n",
       " 'it',\n",
       " 'does',\n",
       " 'not',\n",
       " 'even',\n",
       " 'occur',\n",
       " 'to',\n",
       " 'leo',\n",
       " 'that',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'mystery',\n",
       " 'that',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'be',\n",
       " 'explained',\n",
       " 'about',\n",
       " 'that',\n",
       " '.',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'it',\n",
       " 'does',\n",
       " 'not',\n",
       " 'occur',\n",
       " 'to',\n",
       " 'leo',\n",
       " 'and',\n",
       " 'apparently',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'occur',\n",
       " 'to',\n",
       " 'tim',\n",
       " 'burton',\n",
       " 'either',\n",
       " 'is',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'of',\n",
       " 'the',\n",
       " 'real',\n",
       " 'horror',\n",
       " 'of',\n",
       " 'this',\n",
       " 'film',\n",
       " '.',\n",
       " 'both',\n",
       " 'just',\n",
       " 'assumed',\n",
       " 'that',\n",
       " 'if',\n",
       " 'apes',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'the',\n",
       " 'language',\n",
       " 'they',\n",
       " 'would',\n",
       " 'speak',\n",
       " 'would',\n",
       " 'be',\n",
       " 'english',\n",
       " '.',\n",
       " 'in',\n",
       " 'any',\n",
       " 'case',\n",
       " 'having',\n",
       " 'one',\n",
       " 'talking',\n",
       " 'race',\n",
       " 'dominating',\n",
       " 'another',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'not',\n",
       " 'a',\n",
       " 'look',\n",
       " 'at',\n",
       " 'human',\n",
       " '-',\n",
       " 'animal',\n",
       " 'relationships',\n",
       " 'and',\n",
       " 'more',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'master',\n",
       " '-',\n",
       " 'slave',\n",
       " 'relationships',\n",
       " '.',\n",
       " 'outside',\n",
       " 'of',\n",
       " 'sudan',\n",
       " 'and',\n",
       " 'a',\n",
       " 'few',\n",
       " 'other',\n",
       " 'countries',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'less',\n",
       " 'relevant',\n",
       " 'topic',\n",
       " '.',\n",
       " 'leo',\n",
       " 'is',\n",
       " 'captured',\n",
       " 'to',\n",
       " 'be',\n",
       " 'used',\n",
       " 'as',\n",
       " 'a',\n",
       " 'slave',\n",
       " 'but',\n",
       " 'also',\n",
       " 'is',\n",
       " 'discovered',\n",
       " 'by',\n",
       " 'ari',\n",
       " ',',\n",
       " 'played',\n",
       " 'by',\n",
       " 'helena',\n",
       " 'bonham',\n",
       " 'carter',\n",
       " '.',\n",
       " 'ari',\n",
       " 'is',\n",
       " 'an',\n",
       " 'attractive',\n",
       " 'ape',\n",
       " 'with',\n",
       " 'close',\n",
       " 'ties',\n",
       " 'to',\n",
       " 'high',\n",
       " 'political',\n",
       " 'power',\n",
       " '.',\n",
       " 'she',\n",
       " 'is',\n",
       " 'bent',\n",
       " 'on',\n",
       " 'making',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[200][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     13.4 : 1.0\n",
      "               ludicrous = True              neg : pos    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =      8.9 : 1.0\n",
      "                   jolie = True              neg : pos    =      7.8 : 1.0\n",
      "                   mulan = True              pos : neg    =      7.6 : 1.0\n",
      "                religion = True              pos : neg    =      6.9 : 1.0\n",
      "                  finest = True              pos : neg    =      6.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      6.7 : 1.0\n",
      "             beautifully = True              pos : neg    =      6.5 : 1.0\n",
      "            breathtaking = True              pos : neg    =      6.5 : 1.0\n",
      "                  welles = True              neg : pos    =      6.4 : 1.0\n",
      "              schumacher = True              neg : pos    =      6.3 : 1.0\n",
      "             wonderfully = True              pos : neg    =      6.2 : 1.0\n",
      "                ordinary = True              pos : neg    =      6.0 : 1.0\n",
      "                  alicia = True              neg : pos    =      5.9 : 1.0\n",
      "                  seagal = True              neg : pos    =      5.9 : 1.0\n",
      "                    jedi = True              pos : neg    =      5.7 : 1.0\n",
      "                   inept = True              neg : pos    =      5.7 : 1.0\n",
      "                   hanks = True              pos : neg    =      5.5 : 1.0\n",
      "                 unfunny = True              neg : pos    =      5.4 : 1.0\n",
      "                   damon = True              pos : neg    =      5.4 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.3 : 1.0\n",
      "                    lame = True              neg : pos    =      5.2 : 1.0\n",
      "              ridiculous = True              neg : pos    =      5.1 : 1.0\n",
      "                   bland = True              neg : pos    =      5.1 : 1.0\n",
      "                    bore = True              neg : pos    =      5.1 : 1.0\n",
      "                     sat = True              neg : pos    =      5.1 : 1.0\n",
      "                   waste = True              neg : pos    =      5.0 : 1.0\n",
      "                   anger = True              pos : neg    =      4.9 : 1.0\n",
      "                     gon = True              pos : neg    =      4.9 : 1.0\n",
      "                   awful = True              neg : pos    =      4.9 : 1.0\n",
      "               painfully = True              neg : pos    =      4.8 : 1.0\n",
      "                 luckily = True              pos : neg    =      4.6 : 1.0\n",
      "                  mature = True              pos : neg    =      4.6 : 1.0\n",
      "                   worst = True              neg : pos    =      4.5 : 1.0\n",
      "                  poorly = True              neg : pos    =      4.3 : 1.0\n",
      "               laughable = True              neg : pos    =      4.3 : 1.0\n",
      "                 garbage = True              neg : pos    =      4.3 : 1.0\n",
      "           extraordinary = True              pos : neg    =      4.3 : 1.0\n",
      "                lebowski = True              pos : neg    =      4.3 : 1.0\n",
      "               memorable = True              pos : neg    =      4.3 : 1.0\n",
      "                   badly = True              neg : pos    =      4.2 : 1.0\n",
      "                designer = True              pos : neg    =      4.2 : 1.0\n",
      "                lifeless = True              neg : pos    =      4.1 : 1.0\n",
      "                  boring = True              neg : pos    =      4.1 : 1.0\n",
      "                     era = True              pos : neg    =      4.1 : 1.0\n",
      "                    mess = True              neg : pos    =      4.1 : 1.0\n",
      "                scorsese = True              pos : neg    =      4.1 : 1.0\n",
      "                  random = True              neg : pos    =      4.0 : 1.0\n",
      "               portrayal = True              pos : neg    =      3.9 : 1.0\n",
      "                  harris = True              pos : neg    =      3.9 : 1.0\n",
      "                    dull = True              neg : pos    =      3.9 : 1.0\n",
      "                    snow = True              pos : neg    =      3.9 : 1.0\n",
      "                terrific = True              pos : neg    =      3.9 : 1.0\n",
      "              nomination = True              pos : neg    =      3.8 : 1.0\n",
      "               pointless = True              neg : pos    =      3.7 : 1.0\n",
      "                   notch = True              pos : neg    =      3.7 : 1.0\n",
      "            embarrassing = True              neg : pos    =      3.7 : 1.0\n",
      "                 gabriel = True              neg : pos    =      3.7 : 1.0\n",
      "               excellent = True              pos : neg    =      3.7 : 1.0\n",
      "                   lucas = True              pos : neg    =      3.7 : 1.0\n",
      "                 freddie = True              neg : pos    =      3.6 : 1.0\n",
      "                  paxton = True              pos : neg    =      3.6 : 1.0\n",
      "                   flynt = True              pos : neg    =      3.6 : 1.0\n",
      "                    skip = True              neg : pos    =      3.6 : 1.0\n",
      "                deserves = True              pos : neg    =      3.6 : 1.0\n",
      "                  stupid = True              neg : pos    =      3.5 : 1.0\n",
      "                      na = True              pos : neg    =      3.5 : 1.0\n",
      "                     obi = True              pos : neg    =      3.5 : 1.0\n",
      "                  spacey = True              pos : neg    =      3.5 : 1.0\n",
      "                   virus = True              neg : pos    =      3.5 : 1.0\n",
      "                 decades = True              pos : neg    =      3.5 : 1.0\n",
      "                  snipes = True              neg : pos    =      3.5 : 1.0\n",
      "                   court = True              pos : neg    =      3.4 : 1.0\n",
      "                  flawed = True              pos : neg    =      3.4 : 1.0\n",
      "               realistic = True              pos : neg    =      3.4 : 1.0\n",
      "              whatsoever = True              neg : pos    =      3.4 : 1.0\n",
      "                  tucker = True              pos : neg    =      3.4 : 1.0\n",
      "                 touches = True              pos : neg    =      3.4 : 1.0\n",
      "                  ripley = True              pos : neg    =      3.3 : 1.0\n",
      "                  prinze = True              neg : pos    =      3.3 : 1.0\n",
      "                anywhere = True              neg : pos    =      3.3 : 1.0\n",
      "                  subtle = True              pos : neg    =      3.3 : 1.0\n",
      "                 breasts = True              neg : pos    =      3.3 : 1.0\n",
      "                     eve = True              neg : pos    =      3.3 : 1.0\n",
      "            contemporary = True              pos : neg    =      3.3 : 1.0\n",
      "               endearing = True              pos : neg    =      3.3 : 1.0\n",
      "             masterpiece = True              pos : neg    =      3.3 : 1.0\n",
      "                   damme = True              neg : pos    =      3.3 : 1.0\n",
      "              henstridge = True              neg : pos    =      3.3 : 1.0\n",
      "                godzilla = True              neg : pos    =      3.2 : 1.0\n",
      "                    porn = True              neg : pos    =      3.2 : 1.0\n",
      "               fantastic = True              pos : neg    =      3.2 : 1.0\n",
      "              satisfying = True              pos : neg    =      3.2 : 1.0\n",
      "                  allows = True              pos : neg    =      3.2 : 1.0\n",
      "                 natural = True              pos : neg    =      3.2 : 1.0\n",
      "                 crafted = True              pos : neg    =      3.2 : 1.0\n",
      "                 freedom = True              pos : neg    =      3.2 : 1.0\n",
      "                  german = True              pos : neg    =      3.2 : 1.0\n",
      "                 tedious = True              neg : pos    =      3.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# word_features = [\n",
    "#     w[0] for w in counter_dict.most_common(3000)\n",
    "# ]\n",
    "word_features = classifier1.show_most_informative_features(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features = ['ludicrous','idiotic','seagal','welles','schumacher','inept','lame','whatsoever','wasted','bland','unfunny','alicia',\n",
    "       'sandler','ridiculous','random','jolie','poorly','waste','lifeless','garbage','awful','sat','laughable','bomb',\n",
    "       'stupid','painfully','embarrassing','worst','pointless','bore','mess', 'anger','ecades','finest','damon','anding',\n",
    "       'rfully','mulan','ifully','jesus','nation','flynt','ndship','obi','ligion','tucker','dinary','allows','tastic',\n",
    "       'hanks','bowski','ripley','era','taking','eshing','soners','listic','tiller','lonely','german','gon']\n",
    "\n",
    "len(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Word': 'outstanding',\n",
       " 'Positive/Negative': 'pos : neg',\n",
       " 'Ratio': ' 13.4 : 1.0'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the text file\n",
    "with open('MostInformative.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "data = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "#         print(line.split())\n",
    "        word, _ , _ , *sentiment = line.split()\n",
    "        sentiment = ' '.join(sentiment)\n",
    "        sentiment, ratio = sentiment.split('=')\n",
    "        sentiment = sentiment.strip()\n",
    "        data.append({\n",
    "            'Word': word,\n",
    "            'Positive/Negative': sentiment,\n",
    "            'Ratio': ratio})\n",
    "    \n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outstanding\n",
      "ludicrous\n",
      "refreshing\n",
      "jolie\n",
      "mulan\n",
      "religion\n",
      "finest\n",
      "idiotic\n",
      "beautifully\n",
      "breathtaking\n",
      "welles\n",
      "schumacher\n",
      "wonderfully\n",
      "ordinary\n",
      "alicia\n",
      "seagal\n",
      "jedi\n",
      "inept\n",
      "hanks\n",
      "unfunny\n",
      "damon\n",
      "wasted\n",
      "lame\n",
      "ridiculous\n",
      "bland\n",
      "bore\n",
      "sat\n",
      "waste\n",
      "anger\n",
      "gon\n",
      "awful\n",
      "painfully\n",
      "luckily\n",
      "mature\n",
      "worst\n",
      "poorly\n",
      "laughable\n",
      "garbage\n",
      "extraordinary\n",
      "lebowski\n",
      "memorable\n",
      "badly\n",
      "designer\n",
      "lifeless\n",
      "boring\n",
      "era\n",
      "mess\n",
      "scorsese\n",
      "random\n",
      "portrayal\n",
      "harris\n",
      "dull\n",
      "snow\n",
      "terrific\n",
      "nomination\n",
      "pointless\n",
      "notch\n",
      "embarrassing\n",
      "gabriel\n",
      "excellent\n",
      "lucas\n",
      "freddie\n",
      "paxton\n",
      "flynt\n",
      "skip\n",
      "deserves\n",
      "stupid\n",
      "na\n",
      "obi\n",
      "spacey\n",
      "virus\n",
      "decades\n",
      "snipes\n",
      "court\n",
      "flawed\n",
      "realistic\n",
      "whatsoever\n",
      "tucker\n",
      "touches\n",
      "ripley\n",
      "prinze\n",
      "anywhere\n",
      "subtle\n",
      "breasts\n",
      "eve\n",
      "contemporary\n",
      "endearing\n",
      "masterpiece\n",
      "damme\n",
      "henstridge\n",
      "godzilla\n",
      "porn\n",
      "fantastic\n",
      "satisfying\n",
      "allows\n",
      "natural\n",
      "crafted\n",
      "freedom\n",
      "german\n",
      "tedious\n"
     ]
    }
   ],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for i in data:\n",
    "\n",
    "    print(i['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_features(doc):\n",
    "    words = set(doc)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ludicrous': False,\n",
       " 'idiotic': False,\n",
       " 'seagal': False,\n",
       " 'welles': False,\n",
       " 'schumacher': False,\n",
       " 'inept': False,\n",
       " 'lame': False,\n",
       " 'whatsoever': False,\n",
       " 'wasted': False,\n",
       " 'bland': False,\n",
       " 'unfunny': False,\n",
       " 'alicia': False,\n",
       " 'sandler': False,\n",
       " 'ridiculous': False,\n",
       " 'random': False,\n",
       " 'jolie': False,\n",
       " 'poorly': False,\n",
       " 'waste': False,\n",
       " 'lifeless': False,\n",
       " 'garbage': False,\n",
       " 'awful': False,\n",
       " 'sat': False,\n",
       " 'laughable': False,\n",
       " 'bomb': False,\n",
       " 'stupid': False,\n",
       " 'painfully': False,\n",
       " 'embarrassing': False,\n",
       " 'worst': False,\n",
       " 'pointless': False,\n",
       " 'bore': False,\n",
       " 'mess': True,\n",
       " 'anger': False,\n",
       " 'ecades': False,\n",
       " 'finest': False,\n",
       " 'damon': False,\n",
       " 'anding': False,\n",
       " 'rfully': False,\n",
       " 'mulan': False,\n",
       " 'ifully': False,\n",
       " 'jesus': False,\n",
       " 'nation': False,\n",
       " 'flynt': False,\n",
       " 'ndship': False,\n",
       " 'obi': False,\n",
       " 'ligion': False,\n",
       " 'tucker': False,\n",
       " 'dinary': False,\n",
       " 'allows': False,\n",
       " 'tastic': False,\n",
       " 'hanks': False,\n",
       " 'bowski': False,\n",
       " 'ripley': False,\n",
       " 'era': False,\n",
       " 'taking': False,\n",
       " 'eshing': False,\n",
       " 'soners': False,\n",
       " 'listic': False,\n",
       " 'tiller': False,\n",
       " 'lonely': False,\n",
       " 'german': False,\n",
       " 'gon': False}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_features(docs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = [\n",
    "    (search_features(docs), category)\n",
    "    for (docs, category) in docs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ludicrous': False,\n",
       "  'idiotic': False,\n",
       "  'seagal': False,\n",
       "  'welles': False,\n",
       "  'schumacher': False,\n",
       "  'inept': False,\n",
       "  'lame': False,\n",
       "  'whatsoever': False,\n",
       "  'wasted': False,\n",
       "  'bland': False,\n",
       "  'unfunny': False,\n",
       "  'alicia': False,\n",
       "  'sandler': False,\n",
       "  'ridiculous': False,\n",
       "  'random': False,\n",
       "  'jolie': False,\n",
       "  'poorly': False,\n",
       "  'waste': False,\n",
       "  'lifeless': False,\n",
       "  'garbage': False,\n",
       "  'awful': False,\n",
       "  'sat': False,\n",
       "  'laughable': False,\n",
       "  'bomb': False,\n",
       "  'stupid': False,\n",
       "  'painfully': False,\n",
       "  'embarrassing': False,\n",
       "  'worst': False,\n",
       "  'pointless': False,\n",
       "  'bore': False,\n",
       "  'mess': True,\n",
       "  'anger': False,\n",
       "  'ecades': False,\n",
       "  'finest': False,\n",
       "  'damon': False,\n",
       "  'anding': False,\n",
       "  'rfully': False,\n",
       "  'mulan': False,\n",
       "  'ifully': False,\n",
       "  'jesus': False,\n",
       "  'nation': False,\n",
       "  'flynt': False,\n",
       "  'ndship': False,\n",
       "  'obi': False,\n",
       "  'ligion': False,\n",
       "  'tucker': False,\n",
       "  'dinary': False,\n",
       "  'allows': False,\n",
       "  'tastic': False,\n",
       "  'hanks': False,\n",
       "  'bowski': False,\n",
       "  'ripley': False,\n",
       "  'era': False,\n",
       "  'taking': False,\n",
       "  'eshing': False,\n",
       "  'soners': False,\n",
       "  'listic': False,\n",
       "  'tiller': False,\n",
       "  'lonely': False,\n",
       "  'german': False,\n",
       "  'gon': False},\n",
       " 'neg')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featureset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_features(docs[0][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featureset[:1600]\n",
    "testing_set = featureset[1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Assuming 'my_list' is the list you want to shuffle\n",
    "shuffleList = random.sample(featureset, len(featureset))\n",
    "\n",
    "training_set1 = shuffleList[:1600]\n",
    "testing_set1 = shuffleList[1600:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "negcount = 0\n",
    "poscount = 0\n",
    "for i in range(1600):\n",
    "    if training_set[i][1] == 'neg':\n",
    "        negcount +=1\n",
    "    else:\n",
    "        poscount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total percentage of Negative reviews 62.5\n",
      "Total percentage of Positive reviews 37.5\n"
     ]
    }
   ],
   "source": [
    "print('Total percentage of Negative reviews', (negcount/1600)*100)\n",
    "print('Total percentage of Positive reviews', (poscount/1600)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "negcount = 0\n",
    "poscount = 0\n",
    "for i in range(400):\n",
    "    if testing_set[i][1] == 'neg':\n",
    "        negcount +=1\n",
    "    else:\n",
    "        poscount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total percentage of Negative reviews 0.0\n",
      "Total percentage of Positive reviews 25.0\n"
     ]
    }
   ],
   "source": [
    "print('Total percentage of Negative reviews', (negcount/1600)*100)\n",
    "print('Total percentage of Positive reviews', (poscount/1600)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier's accuracy on testing is : 77.0\n",
      "Classifier's accuracy on training is : 75.5\n"
     ]
    }
   ],
   "source": [
    "classifier1 = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier's accuracy on testing is : {}\".format(nltk.classify.accuracy(classifier1, testing_set)*100))\n",
    "print(\"Classifier's accuracy on training is : {}\".format(nltk.classify.accuracy(classifier1, training_set)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier's accuracy on testing is : 75.75\n",
      "Classifier's accuracy on training is : 76.3125\n"
     ]
    }
   ],
   "source": [
    "classifier1 = nltk.NaiveBayesClassifier.train(training_set1)\n",
    "print(\"Classifier's accuracy on testing is : {}\".format(nltk.classify.accuracy(classifier1, testing_set1)*100))\n",
    "print(\"Classifier's accuracy on training is : {}\".format(nltk.classify.accuracy(classifier1, training_set1)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier's accuracy is : 37.5\n"
     ]
    }
   ],
   "source": [
    "classifier2 = nltk.NaiveBayesClassifier.train(testing_set)\n",
    "print(\"Classifier's accuracy is : {}\".format(nltk.classify.accuracy(classifier2, training_set)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the model accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     13.4 : 1.0\n",
      "               ludicrous = True              neg : pos    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =      8.9 : 1.0\n",
      "                   jolie = True              neg : pos    =      7.8 : 1.0\n",
      "                   mulan = True              pos : neg    =      7.6 : 1.0\n",
      "                religion = True              pos : neg    =      6.9 : 1.0\n",
      "                  finest = True              pos : neg    =      6.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      6.7 : 1.0\n",
      "             beautifully = True              pos : neg    =      6.5 : 1.0\n",
      "            breathtaking = True              pos : neg    =      6.5 : 1.0\n",
      "                  welles = True              neg : pos    =      6.4 : 1.0\n",
      "              schumacher = True              neg : pos    =      6.3 : 1.0\n",
      "             wonderfully = True              pos : neg    =      6.2 : 1.0\n",
      "                ordinary = True              pos : neg    =      6.0 : 1.0\n",
      "                  alicia = True              neg : pos    =      5.9 : 1.0\n",
      "                  seagal = True              neg : pos    =      5.9 : 1.0\n",
      "                    jedi = True              pos : neg    =      5.7 : 1.0\n",
      "                   inept = True              neg : pos    =      5.7 : 1.0\n",
      "                   hanks = True              pos : neg    =      5.5 : 1.0\n",
      "                 unfunny = True              neg : pos    =      5.4 : 1.0\n",
      "                   damon = True              pos : neg    =      5.4 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.3 : 1.0\n",
      "                    lame = True              neg : pos    =      5.2 : 1.0\n",
      "              ridiculous = True              neg : pos    =      5.1 : 1.0\n",
      "                   bland = True              neg : pos    =      5.1 : 1.0\n",
      "                    bore = True              neg : pos    =      5.1 : 1.0\n",
      "                     sat = True              neg : pos    =      5.1 : 1.0\n",
      "                   waste = True              neg : pos    =      5.0 : 1.0\n",
      "                   anger = True              pos : neg    =      4.9 : 1.0\n",
      "                     gon = True              pos : neg    =      4.9 : 1.0\n",
      "                   awful = True              neg : pos    =      4.9 : 1.0\n",
      "               painfully = True              neg : pos    =      4.8 : 1.0\n",
      "                 luckily = True              pos : neg    =      4.6 : 1.0\n",
      "                  mature = True              pos : neg    =      4.6 : 1.0\n",
      "                   worst = True              neg : pos    =      4.5 : 1.0\n",
      "                  poorly = True              neg : pos    =      4.3 : 1.0\n",
      "               laughable = True              neg : pos    =      4.3 : 1.0\n",
      "                 garbage = True              neg : pos    =      4.3 : 1.0\n",
      "           extraordinary = True              pos : neg    =      4.3 : 1.0\n",
      "                lebowski = True              pos : neg    =      4.3 : 1.0\n",
      "               memorable = True              pos : neg    =      4.3 : 1.0\n",
      "                   badly = True              neg : pos    =      4.2 : 1.0\n",
      "                designer = True              pos : neg    =      4.2 : 1.0\n",
      "                lifeless = True              neg : pos    =      4.1 : 1.0\n",
      "                  boring = True              neg : pos    =      4.1 : 1.0\n",
      "                     era = True              pos : neg    =      4.1 : 1.0\n",
      "                    mess = True              neg : pos    =      4.1 : 1.0\n",
      "                scorsese = True              pos : neg    =      4.1 : 1.0\n",
      "                  random = True              neg : pos    =      4.0 : 1.0\n",
      "               portrayal = True              pos : neg    =      3.9 : 1.0\n",
      "                  harris = True              pos : neg    =      3.9 : 1.0\n",
      "                    dull = True              neg : pos    =      3.9 : 1.0\n",
      "                    snow = True              pos : neg    =      3.9 : 1.0\n",
      "                terrific = True              pos : neg    =      3.9 : 1.0\n",
      "              nomination = True              pos : neg    =      3.8 : 1.0\n",
      "               pointless = True              neg : pos    =      3.7 : 1.0\n",
      "                   notch = True              pos : neg    =      3.7 : 1.0\n",
      "            embarrassing = True              neg : pos    =      3.7 : 1.0\n",
      "                 gabriel = True              neg : pos    =      3.7 : 1.0\n",
      "               excellent = True              pos : neg    =      3.7 : 1.0\n",
      "                   lucas = True              pos : neg    =      3.7 : 1.0\n",
      "                 freddie = True              neg : pos    =      3.6 : 1.0\n",
      "                  paxton = True              pos : neg    =      3.6 : 1.0\n",
      "                   flynt = True              pos : neg    =      3.6 : 1.0\n",
      "                    skip = True              neg : pos    =      3.6 : 1.0\n",
      "                deserves = True              pos : neg    =      3.6 : 1.0\n",
      "                  stupid = True              neg : pos    =      3.5 : 1.0\n",
      "                      na = True              pos : neg    =      3.5 : 1.0\n",
      "                     obi = True              pos : neg    =      3.5 : 1.0\n",
      "                  spacey = True              pos : neg    =      3.5 : 1.0\n",
      "                   virus = True              neg : pos    =      3.5 : 1.0\n",
      "                 decades = True              pos : neg    =      3.5 : 1.0\n",
      "                  snipes = True              neg : pos    =      3.5 : 1.0\n",
      "                   court = True              pos : neg    =      3.4 : 1.0\n",
      "                  flawed = True              pos : neg    =      3.4 : 1.0\n",
      "               realistic = True              pos : neg    =      3.4 : 1.0\n",
      "              whatsoever = True              neg : pos    =      3.4 : 1.0\n",
      "                  tucker = True              pos : neg    =      3.4 : 1.0\n",
      "                 touches = True              pos : neg    =      3.4 : 1.0\n",
      "                  ripley = True              pos : neg    =      3.3 : 1.0\n",
      "                  prinze = True              neg : pos    =      3.3 : 1.0\n",
      "                anywhere = True              neg : pos    =      3.3 : 1.0\n",
      "                  subtle = True              pos : neg    =      3.3 : 1.0\n",
      "                 breasts = True              neg : pos    =      3.3 : 1.0\n",
      "                     eve = True              neg : pos    =      3.3 : 1.0\n",
      "            contemporary = True              pos : neg    =      3.3 : 1.0\n",
      "               endearing = True              pos : neg    =      3.3 : 1.0\n",
      "             masterpiece = True              pos : neg    =      3.3 : 1.0\n",
      "                   damme = True              neg : pos    =      3.3 : 1.0\n",
      "              henstridge = True              neg : pos    =      3.3 : 1.0\n",
      "                godzilla = True              neg : pos    =      3.2 : 1.0\n",
      "                    porn = True              neg : pos    =      3.2 : 1.0\n",
      "               fantastic = True              pos : neg    =      3.2 : 1.0\n",
      "              satisfying = True              pos : neg    =      3.2 : 1.0\n",
      "                  allows = True              pos : neg    =      3.2 : 1.0\n",
      "                 natural = True              pos : neg    =      3.2 : 1.0\n",
      "                 crafted = True              pos : neg    =      3.2 : 1.0\n",
      "                 freedom = True              pos : neg    =      3.2 : 1.0\n",
      "                  german = True              pos : neg    =      3.2 : 1.0\n",
      "                 tedious = True              neg : pos    =      3.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier1.show_most_informative_features(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_classifier = open('naive_bayes_model.pkl','wb')\n",
    "pickle.dump(classifier1, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_f = open('naive_bayes_model.pkl','rb')\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "custom_review = 'disgusting  movie'\n",
    "\n",
    "text_filtered = custom_review.translate(str.maketrans('','',string.punctuation))\n",
    "custom_review_tokens = word_tokenize(text_filtered)\n",
    "word_filtered = [w.lower() for w in custom_review_tokens if w.lower() not in stopwords]\n",
    "custom_review_set = search_features(word_filtered)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(classifier.classify(custom_review_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.566, 'pos': 0.434, 'compound': 0.3182}\n",
      "{'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.4374}\n",
      "{'neg': 0.0, 'neu': 0.464, 'pos': 0.536, 'compound': 0.5374}\n",
      "{'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.6371}\n",
      "{'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.4374}\n",
      "{'neg': 0.0, 'neu': 0.634, 'pos': 0.366, 'compound': 0.4374}\n",
      "{'neg': 0.0, 'neu': 0.557, 'pos': 0.443, 'compound': 0.49}\n",
      "{'neg': 0.0, 'neu': 0.548, 'pos': 0.452, 'compound': 0.5106}\n",
      "{'neg': 0.15, 'neu': 0.73, 'pos': 0.121, 'compound': -0.1318}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The car is cool.'\n",
    "sentence1 = 'The car is cool!!'\n",
    "sentence2 = 'The car is cool!!!!!'\n",
    "sentence3 = 'The car is COOL!!!!!'\n",
    "sentence4 = 'THE CAR IS COOL!!'\n",
    "sentence5 = 'THE CAR IS  A BIT COOL!!'\n",
    "sentence6 = 'the car is very cool!!'\n",
    "sentence7 = 'I enjoyed your hotel food.'\n",
    "sentence8 = \"I enjoyed your hotel food, but I didn't like the way waiters offer their services.\"\n",
    "\n",
    "print(analyser.polarity_scores(sentence))\n",
    "print(analyser.polarity_scores(sentence1))\n",
    "print(analyser.polarity_scores(sentence2))\n",
    "print(analyser.polarity_scores(sentence3))\n",
    "print(analyser.polarity_scores(sentence4))\n",
    "print(analyser.polarity_scores(sentence5))\n",
    "print(analyser.polarity_scores(sentence6))\n",
    "print(analyser.polarity_scores(sentence7))\n",
    "print(analyser.polarity_scores(sentence8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preceding Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.477, 'pos': 0.523, 'compound': 0.6588}\n",
      "{'neg': 0.354, 'neu': 0.646, 'pos': 0.0, 'compound': -0.5096}\n"
     ]
    }
   ],
   "source": [
    "sen = 'your hotel sevice is great!'\n",
    "sen1 = 'your hotel service is not that great'\n",
    "\n",
    "print(analyser.polarity_scores(sen))\n",
    "print(analyser.polarity_scores(sen1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
